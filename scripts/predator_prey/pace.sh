#!/bin/bash
python train_.py --env-name MPE \
  --scenario simple_tag_multi_partial --num-agents 4 --num-good-agents 2 --obs-radius 0.2 --horizon 50 --shaped-reward --collide-reward-once --watch-tower \
  --player-id 0 \
  --algo ppo --num-steps 1000 --num-processes 64 --use-gae --eps 1e-8 --use-proper-time-limit \
  --lr 1e-3 --entropy-coef 3e-2 --value-loss-coef 0.5 --num-epochs 4 --num-mini-batch 16 --gamma 0.99 --gae-lambda 0.95 \
  --max-grad-norm 15.0 \
  --num-env-steps 15000000 \
  --save-interval 5000000 --log-interval 10000 --eval-interval 100000 --eval-episodes 5 \
  --exp-name "pace" \
  --latent-training --e2e-obj \
  --deterministic-latent \
  --use-latent-critic \
  --pre-hidden-dims 128 128 --encoder-base mlp --agg-func mean --post-hidden-dims 128 \
  --latent-dim 128 --kl-coef 0 \
  --hidden-dims 128 128 --act-func relu \
  --train-pool-size 16 --eval-pool-size 24 \
  --rule-based-opponents 16 \
  --separate-patterns \
  --auxiliary-policy-cls-coef 1.0 \
  --policy-cls-reward-coef 0.1 --policy-cls-reward-type accuracy --policy-cls-reward-mode full --policy-cls-warmup-steps 500000 \
  --policy-cls-reward-decay-steps 15000000 \
  --visit-reward-type interaction --visit-reward-coef 0.0 \
  --history-middle-sampling \
  --merge-encoder-computation \
  --history-full-size 5 \
  --history-size 5 \
  --history-use-episodes \
  --include-current-episode \
  --use-meta-episode \
  --self-obs-mode \
  --self-action-mode \
  --clear-history-on-full \
  --separate-history \
  --seed $1 \
  --wandb-user-name <wandb_user_name>
